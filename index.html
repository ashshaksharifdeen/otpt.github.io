<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>O-TPT: Orthogonality-Constrained Test-time Prompt Tuning</title>
  <link rel="stylesheet" href="assets/style.css">
</head>
<body>
  <!-- Hero header with gradient background -->
  <!-- HERO HEADER -->
  <header class="hero">
    <div class="hero-content">

          <!-- Highlight badge -->
<div class="highlight">
  <img
    src="assets/images/higlight-icon.svg"
    alt="OTPT Highlight Badge"
    class="badge-icon"
  >
  <span class="highlight-text">Highlighted Paper</span>
</div>

      <!-- Main title -->
      <h1>O-TPT:  Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models</h1>

      <!-- Subtitle -->
      
      <h2 class="title-line2">
       [CVPR ’25]
      </h2>

      <!-- Short description 
      <p class="description">
        Test-time prompt tuning can boost CLIP‐style models’ accuracy on new tasks without labels,
        but often introduces over-confidence. O-TPT enforces angular separation among class prompts
        via an orthogonality regularizer, leading to both high accuracy and well-calibrated confidences.
      </p>-->

      <!-- Authors & Affiliations -->
    <p class="authors">
      <a href="https://scholar.google.com/citations?user=rd9zSX8AAAAJ&hl=en" target="_blank">Ashshak Sharifdeen</a><sup>1,2</sup>,
      <a href="https://scholar.google.com/citations?user=sT-epZAAAAAJ&hl=en" target="_blank">Muhammad Akhtar Munir</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?user=2WJHOBEAAAAJ&hl=en" target="_blank">Sanoojan Baliah</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en" target="_blank">Salman Khan</a><sup>1,3</sup>,
      <a href="https://scholar.google.com/citations?user=ZgERfFwAAAAJ&hl=en" target="_blank">Muhammad Haris Khan</a><sup>1</sup>
    </p>
      <p class="affiliations">
        <sup>1</sup>Mohamed Bin Zayed University of Artificial Intelligence &nbsp;&nbsp;
        <sup>2</sup>University of Colombo &nbsp;&nbsp;
        <sup>3</sup>Australian National University
      </p>
          <!-- Logos -->
    <div class="logos">
      <!-- MBZUAI on its own, bigger -->
      <img src="assets/images/mbzuai_logo.png" alt="MBZUAI Logo" class="logo mbzuai">

      <!-- UoC + ANU side by side below -->
      <div class="other-logos">
        <img src="assets/images/uoc_logo.png" alt="UoC Logo" class="logo small">
        <img src="assets/images/ANU-img.webp" alt="ANU Logo" class="logo small">
      </div>
    </div>
      <!-- Action buttons -->
      <div class="links">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf" target="_blank">
          Paper (CVPR ’25)
        </a>
        <a href="https://github.com/ashshaksharifdeen/O-TPT" target="_blank">Code</a>
        <a href="https://docs.google.com/presentation/d/1aRIgcdLnAUOojj45v39bugMX2F2aYLpY/edit" target="_blank">
          Slides
        </a>
      </div>
    </div>
  </header>

 

  <main>
    
  <!-- Video demo -->
  <section id="video">
    
    <div class="video-container">
      <iframe
        src="https://www.youtube.com/embed/zBh-tCqpGUE"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen>
      </iframe>
    </div>
  </section> 

    <!-- Overview -->
    <section id="overview">
      <h2>Abstract</h2>
      <p>
        Test-time prompt tuning for vision-language models
 (VLMs) is getting attention because of their ability to learn
 with unlabeled data without fine-tuning. Although test
time prompt tuning methods for VLMs can boost accuracy,
 the resulting models tend to demonstrate poor calibration,
 which casts doubts on the reliability and trustworthiness
 of these models. Notably, more attention needs to be de
voted to calibrating the test-time prompt tuning in vision
language models. To this end, we propose a new approach,
 called O-TPT that introduces orthogonality constraints on
 the textual features corresponding to the learnable prompts
 for calibrating test-time prompt tuning in VLMs. Towards
 introducing orthogonality constraints, we make the follow
ing contributions. First, we uncover new insights behind the
 suboptimal calibration performance of existing methods re
lying on textual feature dispersion. Second, we show that
 imposing a simple orthogonalization of textual features is a
 more effective approach towards obtaining textual disper
sion. We conduct extensive experiments on various datasets
 with different backbones and baselines. The results indi
cate that our method consistently outperforms the prior
 state of the art in significantly reducing the overall aver
age calibration error. Also, our method surpasses the zero
shot calibration performance on fine-grained classification
 tasks
      </p>
    </section>



    <!-- Key Contributions with Fig.1 -->
   <section id="contributions">
  <ol>
    <li> 
     <figure>
        <img src="assets/images/radar_plot_3.png" alt="ECE comparison plot">
        <figcaption>Fig.1: ECE vs. baselines</figcaption>
      </figure>
    </li>
  </ol>
</section>

    <!-- Methodology with Fig.3 -->
    <section id="methodology">
      <h2>Orthogonality Constraint</h2>
      <p>
         textual
 features with lower cosine similarity (i.e., greater angular
 separation) between them lead to an improved calibration,
 as indicated by a lower Expected Calibration Error (ECE)
  Armed with this insight, we propose to impose
 orthogonalization constraints on the textual features by en
forcing the angular distance between them. As such, this
 allows us to effectively utilize the feature space. Due to
 improved text feature separation.
      </p>
      <figure>
        <img src="assets/images/stacked_cosine_similarity_pdfs_caltech101_new.png" alt="cosiine simililarity">
        
      </figure>
    </section>

      <section id="Feature-space utilization">
      <h2> Comparison of angular optimization</h2>
      <p>
       Interestingly, C-TPT, which applies dispersion
 in the L2 space, also struggles to calibrate, showing higher
 cosine similarities in cases where TPT fails (these challeng
ing points), as illustrated in Fig. 3. In contrast, our method’s
 orthogonalization constraint consistently produces text fea
tures with much lower and more consistent cosine similari
ties compared to CLIP initialization, resulting in better cali
bration overall. Our orthogonalization method enforces an
gular distance between feature pairs, fully utilizing the hy
perspherical space (Fig. 3). As such, promoting orthogo
nality enhances feature separation, leading to distinct class
 boundaries and improved calibration.
      </p>
      <figure>
        <img src="assets/images/CTPT_vs_ours_diag.png" alt=" Comparison of angular optimization (ours) and ATFD
 optimization ">
        
      </figure>
    </section>  

    <!-- Results with Fig.5 -->
    <section id="ComparisonofcalibrationperformancewithCLIP-ViTB/16backbone">
      <h2>Comparison of calibration performance with CLIP-ViTB/16 backbone</h2>
      <p>
        Using the CLIP-B/16 backbone, our method achieves an
 average ECE of 4.21, outperforming C-TPT at 5.13 and
 Robust Adapt’s best result of 4.66.
      </p>
      <figure>
        <img src="assets/images/clipvit16.png" alt="CLIP-ViTB/16 backbone">
        
      </figure>
    </section>

        <!-- Results with Fig.5 -->
    <section id="ComparisonofcalibrationperformancewithCLIP-RN50backbone">
      <h2>Comparison of calibration performance with CLIP-RN50 backbone</h2>
      <p>
        When applied to the
 CLIP-RN50 backbone, our approach reduces ECE to 5.45,
 a substantial improvement over the 6.19 ECE achieved by
 C-TPT. Notably, our method also surpasses the zero-shot
 calibration performance showing lower ECE on both back
bones
      </p>
      <figure>
        <img src="assets/images/CLIP-RN50.png" alt="ComparisonofcalibrationperformancewithCLIP-RN50backbone">
       
      </figure>
    </section>

        <!-- Results with Fig.5
    <section id="reliability">
      <h2>Reliability Diagrams</h2>
      <p>
         Reliability diagrams display the result for CLIP-ViT-B/16 on the
 Food, Flower, and DTD datasets.  In Fig.a, C-TPT
 shows under confidence for the Food dataset, which is rec
tified withour O-TPT calibration method, as shown in
 Fig.d.Additionally,ourmethod better addresses overcon
fidenc eissues, as illustrated in Fig.e and f,when com
pared to Fig.b and c,respectively.
      </p>
      <figure>
        <img src="assets/images/Reliability.png" alt="reliabilitydai">
        
      </figure>
    </section>  -->

    <!-- Resources 
    <section id="resources">
      <h2>Resources</h2>
      <ul>
        <li><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf" target="_blank">Full paper (CVPR ’25)</a></li>
        <li><a href="https://github.com/ashshaksharifdeen/O-TPT" target="_blank">GitHub: code & experiments</a></li>
        <li><a href="https://docs.google.com/presentation/d/1aRIgcdLnAUOojj45v39bugMX2F2aYLpY/edit" target="_blank">Slides & demo video</a></li>
      </ul>
    </section> -->

     <section id="bibtex">
    <h2>BibTeX</h2>
    <p>If you like our work, please consider citing us.</p>
    <div class="code-block">
      <pre><code>
@misc{sharifdeen2025otptorthogonalityconstraintscalibrating,
  title={O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models},
  author={Ashshak Sharifdeen and Muhammad Akhtar Munir and Sanoojan Baliah and Salman Khan and Muhammad Haris Khan},
  year={2025},
  eprint={2503.12096},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2503.12096},
}
      </code></pre>
    </div>
  </section> 
  </main>

  <footer>
    <p>
      <strong>O-TPT</strong> is maintained by
      <a href="https://github.com/ashshaksharifdeen" target="_blank">ashshak sharifdeen</a>.
    </p>
  </footer>
</body>
</html>

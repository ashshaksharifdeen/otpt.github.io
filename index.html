<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>O-TPT: Orthogonality-Constrained Test-time Prompt Tuning</title>
  <link rel="stylesheet" href="assets/style.css">
</head>
<body>
  <!-- Hero header with gradient background -->
  <!-- HERO HEADER -->
  <header class="hero">
    <div class="hero-content">
      <!-- Main title -->
      <h1>O-TPT:  Orthogonality Constraints for Calibrating Test-time Prompt Tuning<br />
        in Vision-Language Models [CVPR'25]</h1>

      <!-- Subtitle -->
      

      <!-- Short description -->
      <p class="description">
        Test-time prompt tuning can boost CLIP‐style models’ accuracy on new tasks without labels,
        but often introduces over-confidence. O-TPT enforces angular separation among class prompts
        via an orthogonality regularizer, leading to both high accuracy and well-calibrated confidences.
      </p>

      <!-- Authors & Affiliations -->
    <p class="authors">
      <a href="https://scholar.google.com/citations?user=rd9zSX8AAAAJ&hl=en" target="_blank">Ashshak Sharifdeen</a><sup>1,2</sup>,
      <a href="https://scholar.google.com/citations?user=sT-epZAAAAAJ&hl=en" target="_blank">Muhammad Akhtar Munir</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?user=2WJHOBEAAAAJ&hl=en" target="_blank">Sanoojan Baliah</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en" target="_blank">Salman Khan</a><sup>1,3</sup>,
      <a href="https://scholar.google.com/citations?user=ZgERfFwAAAAJ&hl=en" target="_blank">Muhammad Haris Khan</a><sup>1</sup>
    </p>
      <p class="affiliations">
        <sup>1</sup>Mohamed Bin Zayed University of Artificial Intelligence &nbsp;&nbsp;
        <sup>2</sup>University of Colombo &nbsp;&nbsp;
        <sup>3</sup>Australian National University
      </p>

      <!-- Action buttons -->
      <div class="links">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf" target="_blank">
          Paper (CVPR ’25)
        </a>
        <a href="https://github.com/ashshaksharifdeen/O-TPT" target="_blank">Code</a>
        <a href="https://docs.google.com/presentation/d/1aRIgcdLnAUOojj45v39bugMX2F2aYLpY/edit" target="_blank">
          Slides
        </a>
      </div>
    </div>
  </header>

  <main>
    
    <section id="video">
    <h2>Overview Video</h2>
    <div class="video-container">
      <iframe
        src="https://www.youtube.com/embed/zBh-tCqpGUE"
        frameborder="0" allowfullscreen>
      </iframe>
    </div>
  </section>

    <!-- Overview -->
    <section id="overview">
      <h2>Overview</h2>
      <p>
        Test-time prompt tuning (TPT) can boost CLIP-style models’ accuracy on new tasks without labels, but often at the cost of over-confident, mis-calibrated predictions.
        O-TPT introduces an orthogonality regularizer on class-prompt embeddings, enforcing them to span the unit hypersphere with low cosine similarity—improving calibration across benchmarks.
      </p>
    </section>



    <!-- Key Contributions with Fig.1 -->
   <section id="contributions">
  <h2>Key Contributions</h2>
  <ol>
    <li>
      <strong>Orthogonal Prompt Regularizer:</strong>
      Add ∥E Eᵀ − I∥₂² to the TPT loss to enforce angular separation among text prompts.
    </li>
    <li>
      <strong>Comprehensive Evaluation:</strong>
      11 datasets (ImageNet, fine-grained, OOD variants), CLIP ViT-B/16 & RN50 backbones.
    </li>
    <li>
      <strong>SOTA Calibration:</strong>
      O-TPT achieves 4.21% ECE vs. 5.13% (C-TPT) and 12.0% (TPT) on fine-grained tasks.
      <figure>
        <img src="assets/images/radar_plot_3.png" alt="ECE comparison plot">
        <figcaption>Fig.1: ECE vs. baselines</figcaption>
      </figure>
    </li>
  </ol>
</section>

    <!-- Methodology with Fig.3 -->
    <section id="methodology">
      <h2>Methodology</h2>
      <p>
        Let <code>E∈ℝ^{C×D}</code> be the matrix of class-prompt embeddings. We optimize:
      </p>
      <pre><code>
minₜ L_{TPT}(t) + λ · ∥E Eᵀ − I_C∥₂²
      </code></pre>
      <p>where L_{TPT} is the entropy-minimization loss of TPT.</p>
      <figure>
        <img src="assets/images/CTPT_vs_ours_diag.png" alt="Orthogonal vs. dispersion comparison">
        <figcaption>Fig. 3: Orthogonal vs. L₂-dispersion (C-TPT) :contentReference[oaicite:12]{index=12}</figcaption>
      </figure>
    </section>

    <!-- Results with Fig.5 -->
    <section id="results">
      <h2>Results</h2>
      <p>
        Across fine-grained datasets, O-TPT consistently lowers ECE without sacrificing accuracy:
      </p>
      <figure>
        <img src="assets/images/various-method2.png" alt="Fine-grained datasets ECE">
        <figcaption>Fig. 5: ECE across fine-grained tasks (average) :contentReference[oaicite:13]{index=13}</figcaption>
      </figure>
    </section>

    <!-- Reliability Plots -->
    <section id="reliability">
      <h2>Reliability Diagrams</h2>
      <p>
        O-TPT corrects over-confidence on ImageNet-A, Aircraft, UCF101, SUN397:
      </p>
      <ul>
        <li>
          <strong>CLIP ViT-B/16:</strong> Fig. 8 compares C-TPT vs. O-TPT reliability :contentReference[oaicite:14]{index=14}.
        </li>
        <li>
          <strong>CLIP RN-50:</strong> Fig. 9 shows similar gains :contentReference[oaicite:15]{index=15}.
        </li>
      </ul>
    </section>

    <!-- Resources -->
    <section id="resources">
      <h2>Resources</h2>
      <ul>
        <li><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf" target="_blank">Full paper (CVPR ’25)</a></li>
        <li><a href="https://github.com/ashshaksharifdeen/O-TPT" target="_blank">GitHub: code & experiments</a></li>
        <li><a href="https://docs.google.com/presentation/d/1aRIgcdLnAUOojj45v39bugMX2F2aYLpY/edit" target="_blank">Slides & demo video</a></li>
      </ul>
    </section>
  </main>

  <footer>
    <p>© 2025 Ashshak Sharifdeen et al.</p>
  </footer>
</body>
</html>
